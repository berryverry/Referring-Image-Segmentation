{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbfed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dce81f-e020-4a91-89f1-13eea51c9c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5525b3-9c87-4183-b8ba-038b4775415e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from vicas.dataset import ViCaSDataset, ViCaSVideo\n",
    "from vicas.caption_parsing import parse_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da753015-51a6-4171-b1ad-d725e967aede",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset API\n",
    "\n",
    "`ViCaSDataset` is a wrapper class to easily iterate over all videos.\n",
    "\n",
    "**TODO:** Set `annotations_dir` to the directory path where all the JSON annotations are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f863268-a2a6-4503-92cc-7496b0c32c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 5131 videos from the dataset\n"
     ]
    }
   ],
   "source": [
    "annotations_dir = \"/data/dataset/ViCaS/annotations/v0.1\"\n",
    "video_frames_dir = \"/data/dataset/ViCaS/video_frames/\"\n",
    "split = 'train' \n",
    "dataset = ViCaSDataset(\n",
    "    annotations_dir, \n",
    "    split=split,\n",
    "    video_frames_dir=video_frames_dir\n",
    ")\n",
    "print(f\"Indexed {len(dataset)} videos from the dataset\")\n",
    "vid_to_json = dataset.video_id_to_json \n",
    "vid_splits_train = dataset.get_split_videos('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_video_id = 0  \n",
    "video = dataset.parse_video(example_video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c606db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = vid_to_json[example_video_id]\n",
    "import json\n",
    "with open(json_file, 'r') as fh:\n",
    "    content = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tattooed man in shorts is exercising with a rubber band in the bedroom when one end of the rubber band suddenly loosens and hits his crotch. He covers his crotch and walks to the right in pain.\n",
      "[(1, 'A tattooed man in shorts'), (2, 'rubber band'), (2, 'rubber band')]\n"
     ]
    }
   ],
   "source": [
    "caption_raw = content['caption_raw_en']\n",
    "caption_obj = parse_caption(caption_raw)\n",
    "print(caption_obj.parsed)\n",
    "\n",
    "id_phrase_pairs = [(obj.ids[0], obj.phrase) for obj in caption_obj.objects]\n",
    "print(id_phrase_pairs)\n",
    "# [(1, 'A tattooed man in shorts'), (2, 'rubber band'), (2, 'rubber band')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2211320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2}\n"
     ]
    }
   ],
   "source": [
    "instance_ids = []\n",
    "for i in content['object_referrals'] :\n",
    "    instance_ids.extend(i['track_ids'])\n",
    "print(set(instance_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def overlay_mask_on_image(image, mask, mask_opacity=0.6, mask_color=(0, 255, 0), border_thickness=0):\n",
    "    if mask.ndim == 3:\n",
    "        assert mask.shape[2] == 1\n",
    "        _mask = mask.squeeze(axis=2)\n",
    "    else:\n",
    "        _mask = mask\n",
    "\n",
    "    mask_bgr = np.stack((_mask, _mask, _mask), axis=2)\n",
    "    masked_image = np.where(mask_bgr > 0, mask_color, image)\n",
    "    overlayed_image = ((mask_opacity * masked_image) + ((1. - mask_opacity) * image)).astype(np.uint8)\n",
    "\n",
    "    if border_thickness > 0:\n",
    "        _mask = _mask.astype(np.uint8)\n",
    "        assert border_thickness % 2 == 1  # odd number\n",
    "        kernel = np.ones((border_thickness, border_thickness), np.uint8)\n",
    "        edge_mask = cv2.dilate(_mask, kernel, iterations=1) - _mask\n",
    "        edge_mask = np.stack([edge_mask, edge_mask, edge_mask], axis=2)\n",
    "        mask_color = np.array(mask_color, np.uint8)[None, None, :]\n",
    "        mask_color = np.repeat(mask_color, image.shape[0], 0)\n",
    "        mask_color = np.repeat(mask_color, image.shape[1], 1)\n",
    "        overlayed_image = np.where(edge_mask > 0, mask_color, overlayed_image)\n",
    "\n",
    "    return overlayed_image\n",
    "\n",
    "video_frames_dir = \"/data/dataset/ViCaS/video_frames\"\n",
    "for prompt, masks, track_ids, filenames in video.parse_lgvis(include_auto_annotated_masks=False, return_viz=False):\n",
    "    num_frames = len(filenames)\n",
    "    num_bins = 4\n",
    "    bin_size = num_frames // num_bins\n",
    "    selected_indices = []\n",
    "\n",
    "    print(prompt)\n",
    "    print(track_ids)\n",
    "    for i in range(num_bins):\n",
    "        start_idx = i * bin_size\n",
    "        end_idx = (i + 1) * bin_size if i != num_bins - 1 else num_frames\n",
    "        selected_indices.append(random.randint(start_idx + 1, end_idx - 1))\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_bins, figsize=(20, 8))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        frame_path = os.path.join(video_frames_dir, filenames[idx])\n",
    "        image = cv2.imread(frame_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "        mask_list = masks[idx] \n",
    "        if isinstance(mask_list, list):\n",
    "            if len(mask_list) > 0:\n",
    "                mask = mask_list[0] \n",
    "            else:\n",
    "                print(f\"⚠️ Warning: No masks available for frame {idx}\")\n",
    "                continue\n",
    "        else:\n",
    "            mask = mask_list \n",
    "\n",
    "        if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "            mask = mask.squeeze(0)\n",
    "\n",
    "        overlayed_image = overlay_mask_on_image(image, mask, mask_opacity=0.6, mask_color=(255, 0, 0))\n",
    "\n",
    "        axes[i].imshow(overlayed_image)\n",
    "        axes[i].set_title(f\"Frame {idx}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462626cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "os.environ['GEMINI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4804c6",
   "metadata": {},
   "source": [
    "### ViCaS benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fe714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import math\n",
    "import google.generativeai as genai\n",
    "from collections import defaultdict\n",
    "from vicas.caption_parsing import parse_caption\n",
    "\n",
    "def refine_action_based_caption(original_caption, obj_description, frame_name):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are an AI assistant that specializes in generating **concise**, action-centric, human-centered video descriptions.\n",
    "        Your task is to rewrite the given caption to focus on the **main visible action** of the object in the scene.\n",
    "\n",
    "        **Context:**\n",
    "        - The main object is: {obj_description}\n",
    "\n",
    "        **Instructions:**\n",
    "        - Focus on **only one** clear and **visually observable** action the object is performing.\n",
    "        - If multiple actions are described (e.g., \"turns and kicks\"), select the most salient or defining action, and describe it **precisely**.\n",
    "        - Replace vague, mental, or goal-oriented verbs (e.g., \"try,\" \"avoid,\" \"prepare\") with concrete, visible actions.\n",
    "        - Eliminate redundant elements such as consequences, causal structures (\"causing\", \"when\"), or position-only descriptions.\n",
    "        - Avoid brackets (`[]`), slashes (`/`), or placeholders (`<maskX>`).\n",
    "        - Keep the tone objective and the sentence short.\n",
    "\n",
    "        **Original Caption:** {original_caption}\n",
    "\n",
    "        **Now, rewrite the caption to clearly describe the object’s main visible action.**\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        caption = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT-4o API call: {e}\")\n",
    "        return original_caption \n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "def is_movable_object(obj_description):\n",
    "\n",
    "    static_objects = [\"rubber band\", \"table\", \"chair\", \"cup\", \"bottle\", \"lamp\", \"ball\", \"tree\", \"car\", \"building\", \"book\", \"box\", \"door\"]\n",
    "    \n",
    "    human_keywords = [\"man\", \"woman\", \"person\", \"people\", \"child\", \"boy\", \"girl\", \"baby\"]\n",
    "\n",
    "    if \"'s\" in obj_description.lower():\n",
    "        return False\n",
    "\n",
    "    if any(word in obj_description.lower() for word in human_keywords):\n",
    "        return True  \n",
    "\n",
    "    if obj_description.lower() in static_objects:\n",
    "        return False\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Determine if the object is MOVABLE or STATIC.\n",
    "\n",
    "    - MOVABLE: Humans, animals, self-moving objects.\n",
    "    - STATIC: Vehicles, bicycles, objects requiring external force.\n",
    "    - **If the object description contains a person (e.g., \"man in plaid shirts\"), it should always be MOVABLE.**\n",
    "    - **If the object contains \"'s\" (e.g., \"man's shoulder\", \"dog's tail\"), it should always be STATIC.**\n",
    "    - If the object is **clothing (pants, shirt, shoes, socks, etc.), classify it as STATIC.**\n",
    "    - Answer only \"MOVABLE\" or \"STATIC\".\n",
    "\n",
    "    **Object to classify:** {obj_description}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-lite\")\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        if response and response.text:\n",
    "            classification = response.text.strip().upper()\n",
    "            if \"MOVABLE\" in classification:\n",
    "                print(f\"✅ {obj_description} is MOVABLE!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ {obj_description} is STATIC.\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Error: Could not classify {obj_description}, assuming STATIC.\")\n",
    "            return False  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini API 호출 오류: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_object_description_from_caption(id_phrase_pairs, obj_id):\n",
    "    for (caption_obj_id, phrase) in id_phrase_pairs:\n",
    "        if caption_obj_id == obj_id:\n",
    "            return phrase  \n",
    "    return \"Unknown Object\" \n",
    "\n",
    "annotations_dir = \"/data/dataset/ViCaS/annotations/v0.1\"\n",
    "video_frames_dir = \"/data/dataset/ViCaS/video_frames/\"\n",
    "split = 'train'\n",
    "\n",
    "dataset = ViCaSDataset(\n",
    "    annotations_dir, \n",
    "    split=split,\n",
    "    video_frames_dir=video_frames_dir\n",
    ")\n",
    "\n",
    "total_videos = len(dataset.video_id_to_json)\n",
    "sample_size = math.ceil(total_videos / 10)\n",
    "\n",
    "current_batch = 5\n",
    "\n",
    "start_idx = (current_batch - 1) * sample_size\n",
    "end_idx = min(start_idx + sample_size - 1, total_videos - 1)\n",
    "\n",
    "video_ids = list(dataset.video_id_to_json.keys())[start_idx:end_idx]\n",
    "\n",
    "print(f\"✅ Processing batch {current_batch}/10: {start_idx} to {end_idx} (Total {total_videos})\")\n",
    "print(f\"✅ Selected {len(video_ids)} video IDs\")\n",
    "\n",
    "output_json = defaultdict(lambda: defaultdict(dict))\n",
    "samples = []\n",
    "\n",
    "for video_idx, video_id in enumerate(video_ids, start=1):\n",
    "    json_file = dataset.video_id_to_json[video_id]\n",
    "    \n",
    "    with open(json_file, 'r') as fh:\n",
    "        content = json.load(fh)  \n",
    "\n",
    "    caption_raw = content['caption_raw_en']\n",
    "    caption_obj = parse_caption(caption_raw)  \n",
    "    print(f\"\\nProcessing Video {video_idx}/{len(video_ids)} ({(video_idx/len(video_ids))*100:.1f}%) - ID: {video_id}\")\n",
    "    print(f\"Caption: {caption_raw}\")\n",
    "\n",
    "    id_phrase_pairs = [(obj.ids[0], obj.phrase) for obj in caption_obj.objects]\n",
    "    print(f\"Extracted Objects: {id_phrase_pairs}\")\n",
    "\n",
    "    video = dataset.parse_video(video_id)\n",
    "\n",
    "    for frame_idx, frame_info in enumerate(video.parse_lgvis(include_auto_annotated_masks=False, return_viz=False), start=1):\n",
    "        prompt, masks, track_ids, filenames = frame_info\n",
    "\n",
    "        print(f\"Frame {frame_idx} in Video {video_id}, Track IDs: {track_ids}\")\n",
    "\n",
    "        if not track_ids:\n",
    "            print(f\"Warning: No valid track IDs found for Frame {frame_idx}\")\n",
    "            continue\n",
    "\n",
    "        for filename in filenames:\n",
    "            frame_name = os.path.basename(filename)\n",
    "            frame_id = int(frame_name.split('.')[0])\n",
    "\n",
    "            instance_dict = {}\n",
    "            unique_instance_ids = sorted(set(track_ids))\n",
    "            valid_instance_ids = []\n",
    "\n",
    "            for obj_id in unique_instance_ids:\n",
    "                obj_description = get_object_description_from_caption(id_phrase_pairs, obj_id)  \n",
    "                print(f\"Object: {obj_description} (ID: {obj_id})\")\n",
    "\n",
    "                if obj_description == \"Unknown Object\":\n",
    "                    continue  \n",
    "\n",
    "                if is_movable_object(obj_description):  \n",
    "                    valid_instance_ids.append(obj_id)\n",
    "\n",
    "            if not valid_instance_ids:\n",
    "                print(f\"Warning: No valid moving instances found in Frame {frame_id}\")\n",
    "                continue\n",
    "\n",
    "            for obj_id in valid_instance_ids:\n",
    "                obj_description = get_object_description_from_caption(id_phrase_pairs, obj_id)\n",
    "\n",
    "                if not is_movable_object(obj_description):  # STATIC이면 건너뜀\n",
    "                    continue\n",
    "\n",
    "                refined_caption = refine_action_based_caption(caption_raw, obj_description, frame_name)\n",
    "\n",
    "                print(f\"✅ ({video_idx}/{len(video_ids)}) Frame {frame_idx}: Generated Caption: {refined_caption}\")\n",
    "\n",
    "                instance_dict[str(obj_id)] = refined_caption  \n",
    "\n",
    "            if video_id not in output_json:\n",
    "                output_json[video_id] = {}\n",
    "\n",
    "            if frame_id not in output_json[video_id]:\n",
    "                output_json[video_id][frame_id] = {\n",
    "                    \"file_name\": frame_name,\n",
    "                    \"instances\": [],\n",
    "                    \"sentences\": {}\n",
    "                }\n",
    "\n",
    "            output_json[video_id][frame_id][\"instances\"].extend(instance_dict.keys())\n",
    "            output_json[video_id][frame_id][\"sentences\"].update(instance_dict)\n",
    "\n",
    "            samples.append({\n",
    "                \"Video ID\": video_id,\n",
    "                \"Frame ID\": frame_id,\n",
    "                \"Instances\": list(instance_dict.keys()),\n",
    "                \"Action-Focused Captions\": instance_dict\n",
    "            })\n",
    "\n",
    "            print(f\"Progress: {len(samples)} samples processed.\")\n",
    "\n",
    "output_json_path = \"vicas_action_captions_5.json\"\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ JSON saved: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445167a",
   "metadata": {},
   "source": [
    "### ViCaS 각자 파트 json 생성 + 시각화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import textwrap\n",
    "# import numpy as np\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# from IPython.display import display\n",
    "# from vicas.dataset import ViCaSDataset\n",
    "\n",
    "# def overlay_mask_with_labels(image, masks, instance_ids, frame_data, video_id, frame_id, opacity=0.5, frame_index=None, total_frames=None):\n",
    "#     image_np = np.array(image).copy()\n",
    "#     overlay = image_np.copy()\n",
    "#     font = ImageFont.load_default()\n",
    "\n",
    "#     for idx, (mask, inst_id) in enumerate(zip(masks, instance_ids), 1):\n",
    "#         if mask.ndim == 3:\n",
    "#             mask = mask.squeeze()\n",
    "#         color = [np.random.randint(100, 255) for _ in range(3)]\n",
    "#         for c in range(3):\n",
    "#             overlay[:, :, c] = np.where(mask == 1, color[c], overlay[:, :, c])\n",
    "\n",
    "#     overlay_pil = Image.fromarray(overlay)\n",
    "\n",
    "#     overlay_np = np.array(overlay_pil)\n",
    "#     blended = (opacity * overlay_np + (1 - opacity) * image_np).astype(np.uint8)\n",
    "#     overlay_pil = Image.fromarray(blended)\n",
    "#     draw = ImageDraw.Draw(overlay_pil)\n",
    "\n",
    "#     for idx, (mask, inst_id) in enumerate(zip(masks, instance_ids), 1):\n",
    "#         if mask.ndim == 3:\n",
    "#             mask = mask.squeeze()\n",
    "#         ys, xs = np.where(mask == 1)\n",
    "#         if len(xs) > 0 and len(ys) > 0:\n",
    "#             cx, cy = int(np.mean(xs)), int(np.mean(ys))\n",
    "#             draw.text((cx, cy), f\"Instance {inst_id}\", fill=(255, 0, 0), font=font)\n",
    "\n",
    "#     width, height = overlay_pil.size\n",
    "#     if frame_index is not None and total_frames is not None:\n",
    "#         header = f\"Video ID: {video_id} | Frame: {frame_id} ({frame_index + 1} out of {total_frames})\"\n",
    "#     else:\n",
    "#         header = f\"Video ID: {video_id} | Frame: {frame_id}\"\n",
    "#     header_lines = [header]\n",
    "\n",
    "#     for inst_id in frame_data[\"instances\"]:\n",
    "#         sentence = frame_data[\"sentences\"].get(inst_id, \"\")\n",
    "#         wrapped = textwrap.wrap(f\"Instance {inst_id}: {sentence}\", width=80)\n",
    "#         header_lines.extend(wrapped)\n",
    "\n",
    "#     line_height = 15\n",
    "#     padding = 10\n",
    "#     header_height = padding * 2 + line_height * len(header_lines)\n",
    "\n",
    "#     final_image = Image.new(\"RGB\", (width, height + header_height), color=(255, 255, 255))\n",
    "#     final_image.paste(overlay_pil, (0, header_height))\n",
    "\n",
    "#     draw = ImageDraw.Draw(final_image)\n",
    "#     y_offset = padding\n",
    "#     for line in header_lines:\n",
    "#         draw.text((padding, y_offset), line, fill=(0, 0, 0), font=font)\n",
    "#         y_offset += line_height\n",
    "\n",
    "#     return final_image\n",
    "\n",
    "# annotations_dir = \"/data/dataset/ViCaS/annotations/v0.1\"\n",
    "# video_frames_dir = \"/data/dataset/ViCaS/video_frames\"\n",
    "# split = 'train'\n",
    "# json_path = \"vicas_all.json\"\n",
    "# output_dir = \"./vicas_output\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# dataset = ViCaSDataset(annotations_dir, split=split, video_frames_dir=video_frames_dir)\n",
    "\n",
    "# with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     full_data = json.load(f)\n",
    "\n",
    "# video_ids = list(full_data.keys())\n",
    "# total_videos = len(video_ids)\n",
    "# samples_per_part = [600, 1100, 200, 300, 195]\n",
    "# assert sum(samples_per_part) <= total_videos, \"지정한 샘플 수가 전체보다 많습니다.\"\n",
    "\n",
    "# selected_part = int(input(\"파트 번호를 입력하세요 (1 ~ 5): \"))\n",
    "# assert 1 <= selected_part <= 5, \"올바른 파트 번호(1 ~ 5)를 입력하세요.\"\n",
    "\n",
    "# start_idx = sum(samples_per_part[:selected_part - 1])\n",
    "# end_idx = start_idx + samples_per_part[selected_part - 1]\n",
    "# selected_video_ids = video_ids[start_idx:end_idx]\n",
    "\n",
    "# print(f\"총 {total_videos}개 중 {len(selected_video_ids)}개 선택됨 (파트 {selected_part})\")\n",
    "\n",
    "# part_data = {vid: full_data[vid] for vid in selected_video_ids}\n",
    "# output_json_path = f\"vicas_part{selected_part}.json\"\n",
    "# with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(part_data, f, ensure_ascii=False, indent=4)\n",
    "# print(f\"✅ JSON saved: {output_json_path}\")\n",
    "\n",
    "# for vid_idx, vid in enumerate(selected_video_ids, 1): \n",
    "#     video = dataset.parse_video(int(vid))\n",
    "#     frames = part_data[vid]\n",
    "#     sorted_frame_ids = sorted([int(fid) for fid in frames.keys()])\n",
    "\n",
    "#     for frame_index, frame_id in enumerate(sorted_frame_ids):\n",
    "#         frame_data = frames[str(frame_id)]\n",
    "#         file_name = frame_data[\"file_name\"]\n",
    "#         image_path = os.path.join(video_frames_dir, str(vid).zfill(6), file_name)\n",
    "\n",
    "#         if not os.path.exists(image_path):\n",
    "#             print(f\"⚠️ 이미지 없음: {image_path}\")\n",
    "#             continue\n",
    "\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         instances = frame_data[\"instances\"]\n",
    "#         instance_ids = [int(i) for i in instances]\n",
    "\n",
    "#         matched_masks = []\n",
    "#         matched_ids = []\n",
    "#         found = False\n",
    "\n",
    "#         for prompt, masks, track_ids, filenames in video.parse_lgvis(include_auto_annotated_masks=False, return_viz=False):\n",
    "#             for idx, fname in enumerate(filenames):\n",
    "#                 if os.path.basename(fname) == file_name:\n",
    "#                     for inst_id in instance_ids:\n",
    "#                         if inst_id in track_ids:\n",
    "#                             inst_index = track_ids.index(inst_id)\n",
    "#                             matched_masks.append(masks[idx][inst_index])\n",
    "#                             matched_ids.append(inst_id)\n",
    "\n",
    "#                     if matched_masks:\n",
    "#                         vis_img = overlay_mask_with_labels(\n",
    "#                             image, matched_masks, matched_ids,\n",
    "#                             frame_data, vid, frame_id,\n",
    "#                             frame_index=frame_index, total_frames=len(sorted_frame_ids)\n",
    "#                         ,opacity=0.5\n",
    "#                         )\n",
    "#                         display(vis_img)\n",
    "\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             if found:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드는 이미지만 시각화! 실행 x\n",
    "# import json\n",
    "# import os\n",
    "# import textwrap\n",
    "# from IPython.display import display\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# json_path = \"vicas_all.json\"\n",
    "# image_dir = \"/data/dataset/ViCaS/video_frames/\"\n",
    "# output_dir = \"./vicas_output\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     full_data = json.load(f)\n",
    "\n",
    "# video_ids = list(full_data.keys())\n",
    "# total_videos = len(video_ids)\n",
    "\n",
    "# samples_per_part = [600, 1100, 200, 300, 195]\n",
    "# assert sum(samples_per_part) <= total_videos, \"지정한 샘플 수가 전체보다 많습니다.\"\n",
    "\n",
    "# selected_part = int(input(\"파트 번호를 입력하세요 (1 ~ 5): \"))\n",
    "# assert 1 <= selected_part <= 5, \"올바른 파트 번호(1 ~ 5)를 입력하세요.\"\n",
    "\n",
    "# start_idx = sum(samples_per_part[:selected_part - 1])\n",
    "# end_idx = start_idx + samples_per_part[selected_part - 1]\n",
    "# selected_video_ids = video_ids[start_idx:end_idx]\n",
    "\n",
    "# print(f\"총 {total_videos}개 중 {len(selected_video_ids)}개 선택됨 (파트 {selected_part})\")\n",
    "\n",
    "# part_data = {vid: full_data[vid] for vid in selected_video_ids}\n",
    "# output_json_path = f\"vicas_part{selected_part}.json\"\n",
    "# with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(part_data, f, ensure_ascii=False, indent=4)\n",
    "# print(f\"✅ JSON saved: {output_json_path}\")\n",
    "\n",
    "# font = ImageFont.load_default()\n",
    "\n",
    "# for vid_idx, vid in enumerate(selected_video_ids, 1):\n",
    "#     frames = part_data[vid]\n",
    "#     sorted_frame_ids = sorted([int(fid) for fid in frames.keys()])\n",
    "\n",
    "#     for frame_id in sorted_frame_ids:\n",
    "#         frame_data = frames[str(frame_id)]\n",
    "#         file_name = frame_data[\"file_name\"]\n",
    "\n",
    "#         video_id_str = str(vid).zfill(6)\n",
    "#         image_path = os.path.join(image_dir, video_id_str, file_name)\n",
    "\n",
    "#         if not os.path.exists(image_path):\n",
    "#             print(f\"⚠️ 이미지 없음: {image_path}\")\n",
    "#             continue\n",
    "\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         width, height = image.size\n",
    "\n",
    "#         instances = frame_data[\"instances\"]\n",
    "#         sentences = frame_data[\"sentences\"]\n",
    "#         text_lines = [f\"Video ID: {video_id_str} | Frame: {frame_id}\"]\n",
    "\n",
    "#         for inst in instances:\n",
    "#             sentence = sentences[inst]\n",
    "#             wrapped = textwrap.wrap(f\"Instance {inst}: {sentence}\", width=80)\n",
    "#             text_lines.extend(wrapped)\n",
    "\n",
    "#         line_height = 15\n",
    "#         padding = 10\n",
    "#         header_height = padding * 2 + line_height * len(text_lines)\n",
    "\n",
    "#         new_image = Image.new(\"RGB\", (width, height + header_height), color=(255, 255, 255))\n",
    "#         new_image.paste(image, (0, header_height))\n",
    "#         draw = ImageDraw.Draw(new_image)\n",
    "\n",
    "#         y_offset = padding\n",
    "#         for line in text_lines:\n",
    "#             draw.text((padding, y_offset), line, font=font, fill=(0, 0, 0))\n",
    "#             y_offset += line_height\n",
    "\n",
    "#         display(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0dccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples_per_part = [600, 1100, 200, 300, 195]\n",
    "\n",
    "# start_end_indices = []\n",
    "# start_idx = 0\n",
    "\n",
    "# for count in samples_per_part:\n",
    "#     end_idx = start_idx + count\n",
    "#     start_end_indices.append((start_idx, end_idx))\n",
    "#     start_idx = end_idx\n",
    "\n",
    "# for i, (start, end) in enumerate(start_end_indices, 1):\n",
    "#     print(f\"Part {i}: start_idx = {start}, end_idx = {end}, count = {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
